{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76a3d6d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchgeo\n",
      "  Using cached torchgeo-0.6.1-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting perceiver-pytorch\n",
      "  Using cached perceiver_pytorch-0.8.8-py3-none-any.whl.metadata (672 bytes)\n",
      "Collecting einops>=0.3 (from torchgeo)\n",
      "  Using cached einops-0.8.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: fiona>=1.8.21 in c:\\users\\prof.raajramsankaran\\appdata\\roaming\\python\\python310\\site-packages (from torchgeo) (1.8.22)\n",
      "Collecting kornia>=0.7.3 (from torchgeo)\n",
      "  Using cached kornia-0.7.3-py2.py3-none-any.whl.metadata (7.7 kB)\n",
      "Collecting lightly!=1.4.26,>=1.4.5 (from torchgeo)\n",
      "  Using cached lightly-1.5.13-py3-none-any.whl.metadata (37 kB)\n",
      "Collecting lightning!=2.3.*,>=2 (from lightning[pytorch-extra]!=2.3.*,>=2->torchgeo)\n",
      "  Using cached lightning-2.4.0-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: matplotlib>=3.5 in c:\\users\\prof.raajramsankaran\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torchgeo) (3.6.3)\n",
      "Requirement already satisfied: numpy>=1.21.2 in c:\\users\\prof.raajramsankaran\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torchgeo) (1.24.0)\n",
      "Requirement already satisfied: pandas>=1.3.3 in c:\\users\\prof.raajramsankaran\\appdata\\roaming\\python\\python310\\site-packages (from torchgeo) (1.5.3)\n",
      "Requirement already satisfied: pillow>=8.4 in c:\\users\\prof.raajramsankaran\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torchgeo) (9.4.0)\n",
      "Requirement already satisfied: pyproj>=3.3 in c:\\users\\prof.raajramsankaran\\appdata\\roaming\\python\\python310\\site-packages (from torchgeo) (3.4.1)\n",
      "Requirement already satisfied: rasterio<1.4,>=1.3 in c:\\users\\prof.raajramsankaran\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torchgeo) (1.3.10)\n",
      "Collecting rtree>=1 (from torchgeo)\n",
      "  Using cached Rtree-1.3.0-py3-none-win_amd64.whl.metadata (2.1 kB)\n",
      "Collecting segmentation-models-pytorch>=0.2 (from torchgeo)\n",
      "  Using cached segmentation_models_pytorch-0.3.4-py3-none-any.whl.metadata (30 kB)\n",
      "Requirement already satisfied: shapely>=1.8 in c:\\users\\prof.raajramsankaran\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torchgeo) (2.0.2)\n",
      "Requirement already satisfied: timm>=0.4.12 in c:\\users\\prof.raajramsankaran\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torchgeo) (1.0.9)\n",
      "Requirement already satisfied: torch>=1.13 in c:\\users\\prof.raajramsankaran\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torchgeo) (2.4.1+cu124)\n",
      "Collecting torchmetrics>=0.10 (from torchgeo)\n",
      "  Using cached torchmetrics-1.4.3-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: torchvision>=0.14 in c:\\users\\prof.raajramsankaran\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torchgeo) (0.19.1+cu124)\n",
      "Requirement already satisfied: attrs>=17 in c:\\users\\prof.raajramsankaran\\appdata\\roaming\\python\\python310\\site-packages (from fiona>=1.8.21->torchgeo) (22.2.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\prof.raajramsankaran\\appdata\\roaming\\python\\python310\\site-packages (from fiona>=1.8.21->torchgeo) (2022.12.7)\n",
      "Requirement already satisfied: click>=4.0 in c:\\users\\prof.raajramsankaran\\appdata\\roaming\\python\\python310\\site-packages (from fiona>=1.8.21->torchgeo) (8.1.3)\n",
      "Requirement already satisfied: cligj>=0.5 in c:\\users\\prof.raajramsankaran\\appdata\\roaming\\python\\python310\\site-packages (from fiona>=1.8.21->torchgeo) (0.7.2)\n",
      "Requirement already satisfied: click-plugins>=1.0 in c:\\users\\prof.raajramsankaran\\appdata\\roaming\\python\\python310\\site-packages (from fiona>=1.8.21->torchgeo) (1.1.1)\n",
      "Requirement already satisfied: six>=1.7 in c:\\users\\prof.raajramsankaran\\appdata\\roaming\\python\\python310\\site-packages (from fiona>=1.8.21->torchgeo) (1.16.0)\n",
      "Requirement already satisfied: munch in c:\\users\\prof.raajramsankaran\\appdata\\roaming\\python\\python310\\site-packages (from fiona>=1.8.21->torchgeo) (2.5.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\prof.raajramsankaran\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from fiona>=1.8.21->torchgeo) (65.5.0)\n",
      "Collecting kornia-rs>=0.1.0 (from kornia>=0.7.3->torchgeo)\n",
      "  Using cached kornia_rs-0.1.5-cp310-none-win_amd64.whl.metadata (8.9 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\prof.raajramsankaran\\appdata\\roaming\\python\\python310\\site-packages (from kornia>=0.7.3->torchgeo) (23.0)\n",
      "Collecting hydra-core>=1.0.0 (from lightly!=1.4.26,>=1.4.5->torchgeo)\n",
      "  Using cached hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting lightly-utils~=0.0.0 (from lightly!=1.4.26,>=1.4.5->torchgeo)\n",
      "  Using cached lightly_utils-0.0.2-py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\prof.raajramsankaran\\appdata\\roaming\\python\\python310\\site-packages (from lightly!=1.4.26,>=1.4.5->torchgeo) (2.8.2)\n",
      "Requirement already satisfied: requests>=2.23.0 in c:\\users\\prof.raajramsankaran\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from lightly!=1.4.26,>=1.4.5->torchgeo) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.44 in c:\\users\\prof.raajramsankaran\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from lightly!=1.4.26,>=1.4.5->torchgeo) (4.66.5)\n",
      "Collecting pydantic>=1.10.5 (from lightly!=1.4.26,>=1.4.5->torchgeo)\n",
      "  Using cached pydantic-2.9.2-py3-none-any.whl.metadata (149 kB)\n",
      "Collecting pytorch-lightning>=1.0.4 (from lightly!=1.4.26,>=1.4.5->torchgeo)\n",
      "  Using cached pytorch_lightning-2.4.0-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: urllib3>=1.25.3 in c:\\users\\prof.raajramsankaran\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from lightly!=1.4.26,>=1.4.5->torchgeo) (1.26.15)\n",
      "Collecting aenum>=3.1.11 (from lightly!=1.4.26,>=1.4.5->torchgeo)\n",
      "  Using cached aenum-3.1.15-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: PyYAML<8.0,>=5.4 in c:\\users\\prof.raajramsankaran\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from lightning!=2.3.*,>=2->lightning[pytorch-extra]!=2.3.*,>=2->torchgeo) (6.0)\n",
      "Requirement already satisfied: fsspec<2026.0,>=2022.5.0 in c:\\users\\prof.raajramsankaran\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning!=2.3.*,>=2->lightning[pytorch-extra]!=2.3.*,>=2->torchgeo) (2024.2.0)\n",
      "Collecting lightning-utilities<2.0,>=0.10.0 (from lightning!=2.3.*,>=2->lightning[pytorch-extra]!=2.3.*,>=2->torchgeo)\n",
      "  Using cached lightning_utilities-0.11.8-py3-none-any.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: typing-extensions<6.0,>=4.4.0 in c:\\users\\prof.raajramsankaran\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from lightning!=2.3.*,>=2->lightning[pytorch-extra]!=2.3.*,>=2->torchgeo) (4.9.0)\n",
      "Collecting bitsandbytes<1.0,>=0.42.0 (from lightning[pytorch-extra]!=2.3.*,>=2->torchgeo)\n",
      "  Using cached bitsandbytes-0.44.1-py3-none-win_amd64.whl.metadata (3.6 kB)\n",
      "Collecting jsonargparse<5.0,>=4.27.7 (from jsonargparse[signatures]<5.0,>=4.27.7; extra == \"pytorch-extra\"->lightning[pytorch-extra]!=2.3.*,>=2->torchgeo)\n",
      "  Using cached jsonargparse-4.33.2-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting omegaconf<3.0,>=2.2.3 (from lightning[pytorch-extra]!=2.3.*,>=2->torchgeo)\n",
      "  Using cached omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: rich<14.0,>=12.3.0 in c:\\users\\prof.raajramsankaran\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from lightning[pytorch-extra]!=2.3.*,>=2->torchgeo) (13.8.0)\n",
      "Collecting tensorboardX<3.0,>=2.2 (from lightning[pytorch-extra]!=2.3.*,>=2->torchgeo)\n",
      "  Using cached tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\prof.raajramsankaran\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib>=3.5->torchgeo) (1.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\prof.raajramsankaran\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib>=3.5->torchgeo) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\prof.raajramsankaran\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib>=3.5->torchgeo) (4.38.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\prof.raajramsankaran\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib>=3.5->torchgeo) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\prof.raajramsankaran\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib>=3.5->torchgeo) (3.0.9)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\prof.raajramsankaran\\appdata\\roaming\\python\\python310\\site-packages (from pandas>=1.3.3->torchgeo) (2022.7.1)\n",
      "Requirement already satisfied: affine in c:\\users\\prof.raajramsankaran\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from rasterio<1.4,>=1.3->torchgeo) (2.4.0)\n",
      "Requirement already satisfied: snuggs>=1.4.1 in c:\\users\\prof.raajramsankaran\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from rasterio<1.4,>=1.3->torchgeo) (1.4.7)\n",
      "Collecting efficientnet-pytorch==0.7.1 (from segmentation-models-pytorch>=0.2->torchgeo)\n",
      "  Using cached efficientnet_pytorch-0.7.1-py3-none-any.whl\n",
      "Requirement already satisfied: huggingface-hub>=0.24.6 in c:\\users\\prof.raajramsankaran\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from segmentation-models-pytorch>=0.2->torchgeo) (0.24.6)\n",
      "Collecting pretrainedmodels==0.7.4 (from segmentation-models-pytorch>=0.2->torchgeo)\n",
      "  Using cached pretrainedmodels-0.7.4.tar.gz (58 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting timm>=0.4.12 (from torchgeo)\n",
      "  Using cached timm-0.9.7-py3-none-any.whl.metadata (58 kB)\n",
      "Requirement already satisfied: safetensors in c:\\users\\prof.raajramsankaran\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from timm>=0.4.12->torchgeo) (0.4.5)\n",
      "Requirement already satisfied: filelock in c:\\users\\prof.raajramsankaran\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.13->torchgeo) (3.12.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\prof.raajramsankaran\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.13->torchgeo) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\prof.raajramsankaran\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.13->torchgeo) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\prof.raajramsankaran\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.13->torchgeo) (3.1.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\prof.raajramsankaran\\appdata\\roaming\\python\\python310\\site-packages (from click>=4.0->fiona>=1.8.21->torchgeo) (0.4.6)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\prof.raajramsankaran\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning!=2.3.*,>=2->lightning[pytorch-extra]!=2.3.*,>=2->torchgeo) (3.10.5)\n",
      "Collecting antlr4-python3-runtime==4.9.* (from hydra-core>=1.0.0->lightly!=1.4.26,>=1.4.5->torchgeo)\n",
      "  Using cached antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting docstring-parser>=0.15 (from jsonargparse[signatures]<5.0,>=4.27.7; extra == \"pytorch-extra\"->lightning[pytorch-extra]!=2.3.*,>=2->torchgeo)\n",
      "  Using cached docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting typeshed-client>=2.1.0 (from jsonargparse[signatures]<5.0,>=4.27.7; extra == \"pytorch-extra\"->lightning[pytorch-extra]!=2.3.*,>=2->torchgeo)\n",
      "  Using cached typeshed_client-2.7.0-py3-none-any.whl.metadata (7.9 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic>=1.10.5->lightly!=1.4.26,>=1.4.5->torchgeo)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.23.4 (from pydantic>=1.10.5->lightly!=1.4.26,>=1.4.5->torchgeo)\n",
      "  Using cached pydantic_core-2.23.4-cp310-none-win_amd64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\prof.raajramsankaran\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.23.0->lightly!=1.4.26,>=1.4.5->torchgeo) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\prof.raajramsankaran\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.23.0->lightly!=1.4.26,>=1.4.5->torchgeo) (3.4)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\prof.raajramsankaran\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from rich<14.0,>=12.3.0->lightning[pytorch-extra]!=2.3.*,>=2->torchgeo) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\prof.raajramsankaran\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from rich<14.0,>=12.3.0->lightning[pytorch-extra]!=2.3.*,>=2->torchgeo) (2.18.0)\n",
      "Requirement already satisfied: protobuf>=3.20 in c:\\users\\prof.raajramsankaran\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboardX<3.0,>=2.2->lightning[pytorch-extra]!=2.3.*,>=2->torchgeo) (4.25.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\prof.raajramsankaran\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->torch>=1.13->torchgeo) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\prof.raajramsankaran\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy->torch>=1.13->torchgeo) (1.3.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\prof.raajramsankaran\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning!=2.3.*,>=2->lightning[pytorch-extra]!=2.3.*,>=2->torchgeo) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\prof.raajramsankaran\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning!=2.3.*,>=2->lightning[pytorch-extra]!=2.3.*,>=2->torchgeo) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\prof.raajramsankaran\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning!=2.3.*,>=2->lightning[pytorch-extra]!=2.3.*,>=2->torchgeo) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\prof.raajramsankaran\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning!=2.3.*,>=2->lightning[pytorch-extra]!=2.3.*,>=2->torchgeo) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\prof.raajramsankaran\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning!=2.3.*,>=2->lightning[pytorch-extra]!=2.3.*,>=2->torchgeo) (1.11.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in c:\\users\\prof.raajramsankaran\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning!=2.3.*,>=2->lightning[pytorch-extra]!=2.3.*,>=2->torchgeo) (4.0.3)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\prof.raajramsankaran\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from markdown-it-py>=2.2.0->rich<14.0,>=12.3.0->lightning[pytorch-extra]!=2.3.*,>=2->torchgeo) (0.1.2)\n",
      "Collecting importlib-resources>=1.4.0 (from typeshed-client>=2.1.0->jsonargparse[signatures]<5.0,>=4.27.7; extra == \"pytorch-extra\"->lightning[pytorch-extra]!=2.3.*,>=2->torchgeo)\n",
      "  Using cached importlib_resources-6.4.5-py3-none-any.whl.metadata (4.0 kB)\n",
      "Using cached torchgeo-0.6.1-py3-none-any.whl (454 kB)\n",
      "Using cached perceiver_pytorch-0.8.8-py3-none-any.whl (12 kB)\n",
      "Using cached einops-0.8.0-py3-none-any.whl (43 kB)\n",
      "Using cached kornia-0.7.3-py2.py3-none-any.whl (833 kB)\n",
      "Using cached lightly-1.5.13-py3-none-any.whl (808 kB)\n",
      "Using cached lightning-2.4.0-py3-none-any.whl (810 kB)\n",
      "Using cached Rtree-1.3.0-py3-none-win_amd64.whl (377 kB)\n",
      "Using cached segmentation_models_pytorch-0.3.4-py3-none-any.whl (109 kB)\n",
      "Using cached timm-0.9.7-py3-none-any.whl (2.2 MB)\n",
      "Using cached torchmetrics-1.4.3-py3-none-any.whl (869 kB)\n",
      "Using cached aenum-3.1.15-py3-none-any.whl (137 kB)\n",
      "Using cached bitsandbytes-0.44.1-py3-none-win_amd64.whl (121.5 MB)\n",
      "Using cached hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
      "Using cached jsonargparse-4.33.2-py3-none-any.whl (208 kB)\n",
      "Using cached kornia_rs-0.1.5-cp310-none-win_amd64.whl (1.3 MB)\n",
      "Using cached lightly_utils-0.0.2-py3-none-any.whl (6.4 kB)\n",
      "Using cached lightning_utilities-0.11.8-py3-none-any.whl (26 kB)\n",
      "Using cached omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
      "Using cached pydantic-2.9.2-py3-none-any.whl (434 kB)\n",
      "Using cached pydantic_core-2.23.4-cp310-none-win_amd64.whl (1.9 MB)\n",
      "Using cached pytorch_lightning-2.4.0-py3-none-any.whl (815 kB)\n",
      "Using cached tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached docstring_parser-0.16-py3-none-any.whl (36 kB)\n",
      "Using cached typeshed_client-2.7.0-py3-none-any.whl (624 kB)\n",
      "Using cached importlib_resources-6.4.5-py3-none-any.whl (36 kB)\n",
      "Building wheels for collected packages: pretrainedmodels, antlr4-python3-runtime\n",
      "  Building wheel for pretrainedmodels (setup.py): started\n",
      "  Building wheel for pretrainedmodels (setup.py): finished with status 'done'\n",
      "  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-py3-none-any.whl size=61017 sha256=7e1d35d5311b23139cda4c99d4e9767a0cb91869e9a03488bc321821f32d9be3\n",
      "  Stored in directory: c:\\users\\prof.raajramsankaran\\appdata\\local\\pip\\cache\\wheels\\35\\cb\\a5\\8f534c60142835bfc889f9a482e4a67e0b817032d9c6883b64\n",
      "  Building wheel for antlr4-python3-runtime (setup.py): started\n",
      "  Building wheel for antlr4-python3-runtime (setup.py): finished with status 'done'\n",
      "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144578 sha256=a99d2192c44ee6e1109678adf33f5dc98f5b2a50d3cf341610491aadbb9a5148\n",
      "  Stored in directory: c:\\users\\prof.raajramsankaran\\appdata\\local\\pip\\cache\\wheels\\12\\93\\dd\\1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n",
      "Successfully built pretrainedmodels antlr4-python3-runtime\n",
      "Installing collected packages: antlr4-python3-runtime, aenum, tensorboardX, rtree, pydantic-core, omegaconf, lightning-utilities, lightly-utils, kornia-rs, jsonargparse, importlib-resources, einops, docstring-parser, annotated-types, typeshed-client, pydantic, hydra-core, torchmetrics, perceiver-pytorch, kornia, efficientnet-pytorch, bitsandbytes, timm, pretrainedmodels, segmentation-models-pytorch, pytorch-lightning, lightning, lightly, torchgeo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 32] The process cannot access the file because it is being used by another process: 'C:\\\\Users\\\\Prof.RaajRamsankaran\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\Lib\\\\site-packages\\\\kornia\\\\losses\\\\cauchy.py'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install torchgeo perceiver-pytorch evaluate datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51e3a0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchgeo.datasets import MillionAID\n",
    "import torchvision.transforms as transforms\n",
    "from transformers import PerceiverForImageClassificationLearned\n",
    "from torch.utils.data import random_split\n",
    "from transformers import AdamW\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm.notebook import tqdm\n",
    "import evaluate\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19885211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations for the data\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),  # Convert to PIL Image\n",
    "    transforms.Resize((224, 224)),  # Resize images to 224x224\n",
    "    transforms.ToTensor(),  # Convert back to Tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalization\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a8e9f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to apply transform directly to the dataset\n",
    "class CustomMillionAID(MillionAID):\n",
    "    def __getitem__(self, index):\n",
    "        data = super().__getitem__(index)\n",
    "        data['image'] = transform(data['image'])\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b2c9e2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"--------------\",device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b0f710d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "#device='cpu'\n",
    "allocated_memory=torch.cuda.memory_allocated()\n",
    "print(allocated_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c0eccac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "train_ds = CustomMillionAID(root='./data', task=\"multi-class\", split=\"train\")\n",
    "# Define the split ratio (e.g., 80% for training, 20% for testing)\n",
    "train_size = int(0.8 * len(train_ds))\n",
    "test_size = len(train_ds) - train_size\n",
    "# Split the dataset\n",
    "train_dataset, test_dataset = random_split(train_ds, [train_size, test_size])\n",
    "# Define DataLoader for the training and test sets\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=2, shuffle=True, num_workers=0)#, pin_memory=True)\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=2, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fa0aa6fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 {'image': tensor([[[[-1.1247, -1.3302, -1.3987,  ..., -0.5424, -0.4568, -0.6109],\n",
      "          [-1.5699, -1.3644, -1.1589,  ..., -0.4568, -0.4397, -0.5767],\n",
      "          [-1.7240, -1.4672, -1.2103,  ..., -0.4397, -0.4739, -0.5596],\n",
      "          ...,\n",
      "          [ 0.1597,  0.1597,  0.1939,  ..., -0.0116, -0.0287, -0.0801],\n",
      "          [ 0.1426,  0.1768,  0.1939,  ..., -0.0801, -0.1143, -0.1314],\n",
      "          [ 0.0741,  0.1426,  0.1768,  ..., -0.1999, -0.2513, -0.1999]],\n",
      "\n",
      "         [[-0.6352, -0.8452, -0.9153,  ..., -0.1099, -0.0049, -0.1450],\n",
      "          [-1.1078, -0.8978, -0.6702,  ..., -0.0224,  0.0126, -0.1275],\n",
      "          [-1.3004, -1.0378, -0.7752,  ..., -0.0049, -0.0399, -0.1275],\n",
      "          ...,\n",
      "          [ 0.1176,  0.1176,  0.1527,  ...,  0.0126, -0.0049, -0.0574],\n",
      "          [ 0.1001,  0.1352,  0.1527,  ..., -0.0224, -0.0399, -0.0574],\n",
      "          [ 0.0301,  0.1001,  0.1352,  ..., -0.1099, -0.1625, -0.1099]],\n",
      "\n",
      "         [[-0.4450, -0.6541, -0.7238,  ...,  0.1476,  0.2348,  0.0953],\n",
      "          [-0.8981, -0.7064, -0.4798,  ...,  0.2348,  0.2522,  0.1128],\n",
      "          [-1.0898, -0.8284, -0.5495,  ...,  0.2522,  0.2173,  0.1302],\n",
      "          ...,\n",
      "          [ 0.1825,  0.1825,  0.2173,  ...,  0.0605,  0.0431, -0.0092],\n",
      "          [ 0.1651,  0.1999,  0.2173,  ...,  0.0256, -0.0092, -0.0267],\n",
      "          [ 0.0953,  0.1651,  0.1999,  ..., -0.0964, -0.1487, -0.0964]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1083,  0.1597,  0.0398,  ..., -0.7650, -0.6281, -0.5767],\n",
      "          [-0.0458,  0.0227, -0.0629,  ..., -0.5938, -0.3883, -0.4568],\n",
      "          [-0.1657, -0.0972, -0.0972,  ..., -0.5082, -0.9534, -1.1589],\n",
      "          ...,\n",
      "          [ 0.0056,  0.1254,  0.1939,  ...,  0.0741,  0.2111, -0.1314],\n",
      "          [ 0.0398,  0.1426,  0.1597,  ..., -0.1999, -0.1828, -0.2171],\n",
      "          [ 0.1426,  0.1939,  0.1597,  ..., -0.3369, -0.4911, -0.6965]],\n",
      "\n",
      "         [[ 0.0126,  0.0826, -0.0049,  ..., -0.6877, -0.5826, -0.5476],\n",
      "          [-0.1275, -0.0399, -0.1099,  ..., -0.5126, -0.3375, -0.4076],\n",
      "          [-0.2150, -0.1450, -0.1099,  ..., -0.4251, -0.8978, -1.1078],\n",
      "          ...,\n",
      "          [ 0.0301,  0.1527,  0.2227,  ...,  0.1702,  0.3102, -0.0399],\n",
      "          [ 0.0651,  0.1702,  0.1877,  ..., -0.1099, -0.0924, -0.1275],\n",
      "          [ 0.1702,  0.2227,  0.1877,  ..., -0.2500, -0.4076, -0.6176]],\n",
      "\n",
      "         [[-0.0615,  0.0082, -0.1138,  ..., -0.6367, -0.4973, -0.4275],\n",
      "          [-0.2184, -0.1312, -0.2184,  ..., -0.4450, -0.2358, -0.2707],\n",
      "          [-0.3055, -0.2532, -0.2184,  ..., -0.3404, -0.7761, -0.9678],\n",
      "          ...,\n",
      "          [-0.1661, -0.0441,  0.0256,  ..., -0.0267,  0.0953, -0.2358],\n",
      "          [-0.1312, -0.0267, -0.0092,  ..., -0.3055, -0.2881, -0.3230],\n",
      "          [-0.0267,  0.0256, -0.0092,  ..., -0.4275, -0.5844, -0.7936]]]]), 'label': tensor([[45],\n",
      "        [49]])}\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(trainloader,0):\n",
    "    print (i, data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c3ffcb10",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PerceiverForImageClassificationLearned were not initialized from the model checkpoint at deepmind/vision-perceiver-learned and are newly initialized because the shapes did not match:\n",
      "- perceiver.decoder.decoder.final_layer.weight: found shape torch.Size([1000, 1024]) in the checkpoint and torch.Size([51, 1024]) in the model instantiated\n",
      "- perceiver.decoder.decoder.final_layer.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([51]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PerceiverForImageClassificationLearned(\n",
       "  (perceiver): PerceiverModel(\n",
       "    (input_preprocessor): PerceiverImagePreprocessor(\n",
       "      (convnet_1x1): Conv2d(3, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (position_embeddings): PerceiverTrainablePositionEncoding()\n",
       "      (positions_projection): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (conv_after_patches): Identity()\n",
       "    )\n",
       "    (embeddings): PerceiverEmbeddings()\n",
       "    (encoder): PerceiverEncoder(\n",
       "      (cross_attention): PerceiverLayer(\n",
       "        (attention): PerceiverAttention(\n",
       "          (self): PerceiverSelfAttention(\n",
       "            (layernorm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (layernorm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (query): Linear(in_features=1024, out_features=512, bias=True)\n",
       "            (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): PerceiverSelfOutput(\n",
       "            (dense): Linear(in_features=512, out_features=1024, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): PerceiverMLP(\n",
       "          (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "          (dense2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (self_attends): ModuleList(\n",
       "        (0-5): 6 x PerceiverLayer(\n",
       "          (attention): PerceiverAttention(\n",
       "            (self): PerceiverSelfAttention(\n",
       "              (layernorm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (layernorm2): Identity()\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): PerceiverSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): PerceiverMLP(\n",
       "            (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "            (dense2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (decoder): PerceiverClassificationDecoder(\n",
       "      (decoder): PerceiverBasicDecoder(\n",
       "        (output_position_encodings): PerceiverTrainablePositionEncoding()\n",
       "        (positions_projection): Identity()\n",
       "        (decoding_cross_attention): PerceiverLayer(\n",
       "          (attention): PerceiverAttention(\n",
       "            (self): PerceiverSelfAttention(\n",
       "              (layernorm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (layernorm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): PerceiverSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): PerceiverMLP(\n",
       "            (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "            (dense2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (final_layer): Linear(in_features=1024, out_features=51, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = PerceiverForImageClassificationLearned.from_pretrained(\"deepmind/vision-perceiver-learned\",\n",
    "                                                               num_labels=51,\n",
    "                                                               ignore_mismatched_sizes=True)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "839bda33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory to save the model\n",
    "save_dir = \"E:\\danny_dl_models\\saved_models\"\n",
    "os.makedirs(save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "760e963b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Prof.RaajRamsankaran\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Total number of training steps\n",
    "total_steps = len(trainloader) * 10  # 10 epochs\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=total_steps, eta_min=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9f2b6cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "def train(model, train_loader, optimizer, device, num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    output_filename = 'G:\\danny\\saved_models_perceiver\\logs.txt'\n",
    "\n",
    "    with open(output_filename, 'w') as f:\n",
    "        for epoch in range(num_epochs):\n",
    "            print(\"Epoch:\", epoch)\n",
    "            #f.write('Epoch' + ' ' + epoch + '\\n')\n",
    "            for batch in tqdm(trainloader):\n",
    "                # get the inputs;\n",
    "                inputs = batch['image'].to(device)\n",
    "                labels = batch['label'].to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward + backward + optimize\n",
    "                outputs = model(inputs=inputs, labels=labels)\n",
    "                loss = outputs.loss\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # evaluate\n",
    "                predictions = outputs.logits.argmax(-1).cpu().detach().numpy()\n",
    "                accuracy = accuracy_score(y_true=batch[\"label\"].numpy(), y_pred=predictions)\n",
    "                f.write(f'epoch {epoch} Loss  {loss.item()} Accuracy {accuracy} \\n')\n",
    "                f.flush()\n",
    "                #print(f\"Loss: {loss.item()}, Accuracy: {accuracy}\")\n",
    "            \n",
    "            scheduler.step()\n",
    "            # Save model after each epoch\n",
    "            model_save_path = os.path.join(save_dir, f'model_epoch{epoch+1}.pth')\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict()\n",
    "            }, model_save_path)\n",
    "            print(f\"Model saved after epoch {epoch+1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "04ee433f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of epochs\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5715b85a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63849951123441cc8c1090f8b56dc3b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Prof.RaajRamsankaran\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PIL\\Image.py:3167: DecompressionBombWarning: Image size (110124036 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved after epoch 1\n",
      "Epoch: 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b052becfdf9b464d8003c7b649280474",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Prof.RaajRamsankaran\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PIL\\Image.py:3167: DecompressionBombWarning: Image size (110124036 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved after epoch 2\n",
      "Epoch: 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21fc1775897f48aa832aa4262b52b795",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Prof.RaajRamsankaran\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PIL\\Image.py:3167: DecompressionBombWarning: Image size (110124036 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved after epoch 3\n",
      "Epoch: 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c03e7af86b5749238db63b52c1d70de7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Prof.RaajRamsankaran\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PIL\\Image.py:3167: DecompressionBombWarning: Image size (110124036 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved after epoch 4\n",
      "Epoch: 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c86641069d744b719519be1dcff66c9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Prof.RaajRamsankaran\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PIL\\Image.py:3167: DecompressionBombWarning: Image size (110124036 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved after epoch 5\n",
      "Epoch: 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7322fd758759403f8b34c9bc1dc70507",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Prof.RaajRamsankaran\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PIL\\Image.py:3167: DecompressionBombWarning: Image size (110124036 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved after epoch 6\n",
      "Epoch: 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27f228e53d4841779b8a8cb418875002",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Prof.RaajRamsankaran\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PIL\\Image.py:3167: DecompressionBombWarning: Image size (110124036 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved after epoch 7\n",
      "Epoch: 7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eee25a5378014ff1b6653e3afa7d3f13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Prof.RaajRamsankaran\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PIL\\Image.py:3167: DecompressionBombWarning: Image size (110124036 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved after epoch 8\n",
      "Epoch: 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13366b457fe14b648e62dcbebb2ae210",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Prof.RaajRamsankaran\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PIL\\Image.py:3167: DecompressionBombWarning: Image size (110124036 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved after epoch 9\n",
      "Epoch: 9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92522dce848e46d092eaf43053e8b769",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Prof.RaajRamsankaran\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PIL\\Image.py:3167: DecompressionBombWarning: Image size (110124036 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved after epoch 10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b894d8c91b041c6855dfd5566bbb697",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Prof.RaajRamsankaran\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PIL\\Image.py:3167: DecompressionBombWarning: Image size (93508900 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: {'accuracy': 0.8945}\n"
     ]
    }
   ],
   "source": [
    "train(model, trainloader, optimizer, device, num_epochs)\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "model.eval()\n",
    "for batch in tqdm(testloader):\n",
    "      # get the inputs; \n",
    "      inputs = batch[\"image\"].to(device)\n",
    "      labels = batch[\"label\"].to(device)\n",
    "\n",
    "      # forward pass\n",
    "      outputs = model(inputs=inputs, labels=labels)\n",
    "      logits = outputs.logits \n",
    "      predictions = logits.argmax(-1).cpu().detach().numpy()\n",
    "      references = batch[\"label\"].numpy()\n",
    "      accuracy.add_batch(predictions=predictions, references=references)\n",
    "\n",
    "final_score = accuracy.compute()\n",
    "print(\"Accuracy on test set:\", final_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
